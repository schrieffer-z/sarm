import os
import subprocess
import multiprocessing
import argparse
from pathlib import Path
import time

# åŸºç¡€é…ç½®
BASE_DIR = Path(os.getcwd()).absolute()
CONFIGS = {
    'judge_bench': {
        'cwd': BASE_DIR / "eval/judge_bench",
        'command_template': "CUDA_VISIBLE_DEVICES={gpu} python judge_bench.py " \
                            "--judge_name reward_model " \
                            "--judge_model {model_path} " \
                            "--pairs data/dataset=judgebench,response_model=gpt-4o-2024-05-13.jsonl"
    },
    'reward_bench': {
        'cwd': BASE_DIR / "eval/reward_bench",
        'command_template': "CUDA_VISIBLE_DEVICES={gpu} python run_v2.py " \
                            "--batch_size=1 " \
                            "--model {model_path} " \
                            "--dataset /NAS/zhangsy/datasets/allenai/reward-bench-2"
    },
    'rm_bench': {
        'cwd': BASE_DIR / "eval/rm_bench",
        'command_template': "CUDA_VISIBLE_DEVICES={gpu} python run_rm.py " \
                            "--model {model_path} " \
                            "--datapath data/total_dataset.json " \
                            "--batch_size 1 " \
                            "--trust_remote_code " \
                            "--chat_template tulu"
    }
}

def run_eval_task(model_path, benchmark, gpu_id):
    """è¿è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡"""
    config = CONFIGS[benchmark]
    cmd = config['command_template'].format(gpu=gpu_id, model_path=model_path)
    
    print(f"ğŸš€ Starting {benchmark} on GPU:{gpu_id} with model: {model_path}")
    print(f"ğŸ“ Working dir: {config['cwd']}")
    print(f"ğŸ’» Command: {cmd}")
    
    start_time = time.time()
    try:
        result = subprocess.run(
            cmd,
            cwd=config['cwd'],
            shell=True,
            check=True,
            text=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )
        status = "âœ… SUCCESS"
        output = result.stdout
    except subprocess.CalledProcessError as e:
        status = "âŒ FAILED"
        output = e.stdout if e.stdout else str(e)
    
    elapsed = time.time() - start_time
    print(f"{status} {benchmark} on {model_path} [{elapsed:.1f}s]\n")
    
    log_entry = {
        'benchmark': benchmark,
        'model': model_path,
        'gpu': gpu_id,
        'status': status,
        'time': elapsed,
        'output': output
    }
    
    return log_entry

def process_checkpoint(model_path):
    """å¤„ç†å•ä¸ªcheckpointçš„ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡"""
    print(f"\n{'='*80}\nğŸ Starting evaluation for {model_path}\n{'='*80}")
    gpu_map = {'judge_bench': 0, 'reward_bench': 1, 'rm_bench': 2}
    
    with multiprocessing.Pool(processes=3) as pool:
        futures = []
        for benchmark, gpu_id in gpu_map.items():
            futures.append(
                pool.apply_async(
                    run_eval_task, 
                    args=(model_path, benchmark, gpu_id)
                )
            )
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [f.get() for f in futures]
    
    print(f"\n{'='*80}\nğŸ Completed evaluation for {model_path}\n{'='*80}\n")
    return results

def main(model_paths, log_file):
    """ä¸»å¤„ç†å‡½æ•°"""
    all_results = []
    
    # å¤„ç†æ‰€æœ‰checkpoints
    for model_path in model_paths:
        results = process_checkpoint(model_path)
        all_results.extend(results)
    
    # ä¿å­˜æ—¥å¿—
    save_logs(all_results, log_file)

def save_logs(logs, log_file):
    """ä¿å­˜è¯„ä¼°æ—¥å¿—åˆ°æ–‡ä»¶"""
    with open(log_file, 'w') as f:
        f.write("Evaluation Report\n")
        f.write(f"Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"Evaluated {len(logs)//3} checkpoint(s): {', '.join({log['model'] for log in logs})}\n\n")
        
        f.write("DETAILED RESULTS:\n")
        f.write("-" * 100 + "\n")
        f.write("MODEL PATH | BENCHMARK | GPU | STATUS | TIME(s) | OUTPUT\n")
        f.write("-" * 100 + "\n")
        
        for log in logs:
            # ç¼©çŸ­é•¿è·¯å¾„æ˜¾ç¤º
            model_display = log['model'] if len(log['model']) < 60 else log['model'][:30] + "..." + log['model'][-27:]
            
            # åˆ›å»ºè¾“å‡ºæ‘˜è¦
            output_summary = log['output'].strip()
            if len(output_summary) > 500:
                output_summary = output_summary[:200] + f"\n... [TRUNCATED: {len(log['output'])} chars total] ...\n" + output_summary[-200:]
            
            f.write(
                f"{model_display} | {log['benchmark']:12} | {log['gpu']} | "
                f"{log['status']} | {log['time']:7.1f} | "
                f"{output_summary}\n"
            )
            f.write("-" * 100 + "\n")
    
    print(f"\nâœ… Evaluation completed! Results saved to {log_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Batch evaluation of checkpoints on three benchmarks.')
    parser.add_argument('--checkpoints', nargs='+', required=True,
                        help='List of checkpoint paths to evaluate (supports glob patterns)')
    parser.add_argument('--log', default='eval_report.txt',
                        help='Log file to save evaluation results (default: eval_report.txt)')
    
    args = parser.parse_args()
    
    # å±•å¼€å¯èƒ½çš„globæ¨¡å¼
    expanded_paths = []
    for pattern in args.checkpoints:
        if '*' in pattern or '?' in pattern:
            expanded_paths.extend(sorted([str(p) for p in Path().glob(pattern)]))
        else:
            expanded_paths.append(pattern)
    
    if not expanded_paths:
        print("âš ï¸ No valid checkpoints found. Exiting.")
        exit(1)
    
    print(f"ğŸ“‹ Found {len(expanded_paths)} checkpoint(s) to evaluate:")
    for path in expanded_paths:
        print(f"  - {path}")
    
    main(expanded_paths, args.log)


# import json
# import csv
# import os
# from collections import defaultdict
# from glob import glob

# # è®¾ç½®ç»“æœç›®å½•å’Œè¾“å‡ºæ–‡ä»¶
# base_dir = "/path/to/your/checkpoints"  # æ›´æ”¹ä¸ºæ‚¨çš„ç›®å½•è·¯å¾„
# output_file = "all_benchmark_results.csv"

# # ä½¿ç”¨defaultdictè‡ªåŠ¨åˆ›å»ºåµŒå¥—å­—å…¸
# results = defaultdict(lambda: defaultdict(dict))

# # éå†æ‰€æœ‰checkpointç›®å½•
# for checkpoint_dir in glob(os.path.join(base_dir, "checkpoint-*")):
#     checkpoint_id = os.path.basename(checkpoint_dir)
    
#     # æ¯ä¸ªæ£€æŸ¥ç‚¹çš„ä¸‰ç§benchmarkæ–‡ä»¶
#     benchmark_files = {
#         "judge": "judgebench.json",
#         "reward": "reward_benchv2.json",
#         "rm": "rm_bench.json"
#     }
    
#     # è¯»å–å¹¶å¤„ç†æ¯ä¸ªbenchmarkæ–‡ä»¶
#     for bench_type, filename in benchmark_files.items():
#         filepath = os.path.join(checkpoint_dir, filename)
        
#         if os.path.exists(filepath):
#             with open(filepath, "r") as f:
#                 try:
#                     data = json.load(f)
#                     # å°†æ–‡ä»¶ä¸­çš„modelå­—æ®µç»Ÿä¸€è®¾ç½®ä¸ºcheckpointç›®å½•å
#                     if data.get("model"):
#                         data["model"] = checkpoint_id
#                     # ç§»é™¤æ¨¡å‹ç±»å‹ç­‰å¯èƒ½å†²çªçš„é”®
#                     data.pop("model_type", None)
#                     data.pop("chat_template", None)
                    
#                     # å°†æ•°æ®å­˜å…¥ç»“æœå­—å…¸ï¼ŒåŠ ä¸Šå‰ç¼€é˜²æ­¢é”®å†²çª
#                     for key, value in data.items():
#                         if key != "model":  # è·³è¿‡æ¨¡å‹åç§°é”®
#                             # æ·»åŠ benchmarkç±»å‹å‰ç¼€
#                             new_key = f"{bench_type}_{key}"
#                             results[checkpoint_id][new_key] = value
#                 except json.JSONDecodeError:
#                     print(f"Error decoding JSON in {filepath}")
#                 except Exception as e:
#                     print(f"Error processing {filepath}: {str(e)}")
#         else:
#             print(f"File not found: {filepath}")

# # å‡†å¤‡CSVè¾“å‡º
# csv_rows = []
# all_columns = set()

# # æ”¶é›†æ‰€æœ‰å”¯ä¸€åˆ—å
# for checkpoint, benchmarks in results.items():
#     # æ·»åŠ åŸºå‡†åˆ—ï¼šæ¨¡å‹åç§°
#     row = {"model": checkpoint}
    
#     # æ·»åŠ å„benchmarkç»“æœ
#     for col, value in benchmarks.items():
#         row[col] = value
#         all_columns.add(col)
    
#     csv_rows.append(row)

# # ç¡®ä¿æ‰€æœ‰åˆ—åæŒ‰å­—æ¯é¡ºåºæ’åº
# all_columns = sorted(all_columns)
# csv_columns = ["model"] + all_columns

# # å†™å…¥CSVæ–‡ä»¶
# with open(output_file, "w", newline="") as csvfile:
#     writer = csv.DictWriter(csvfile, fieldnames=csv_columns)
#     writer.writeheader()
    
#     for row_dict in csv_rows:
#         # ç¡®ä¿æ¯ä¸€è¡ŒåŒ…å«æ‰€æœ‰åˆ—ï¼Œç¼ºå¤±å€¼è®¾ä¸ºç©ºå­—ç¬¦ä¸²
#         full_row = {col: row_dict.get(col, "") for col in csv_columns}
#         writer.writerow(full_row)

# print(f"âœ… æ‰€æœ‰benchmarkç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(output_file)}")
# print(f"å…±å¤„ç†äº† {len(csv_rows)} ä¸ªæ¨¡å‹")