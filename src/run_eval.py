import os
import re
import argparse
import subprocess
import pandas as pd
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

# é…ç½®è¯„ä¼°ä»»åŠ¡ç›®å½•
BENCH_DIRS = {
    'judge_bench': '/data/zhangsy/JudgeBench/',
    'reward_bench': '/data/zhangsy/eval/reward_bench/',
    'rm_bench': '/data/zhangsy/eval/rm_bench/'
}

def run_benchmark(checkpoint_path, benchmark, gpu_id):
    """è¿è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡å¹¶è¿”å›è¾“å‡º"""
    try:
        command = []
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        
        if benchmark == 'judge_bench':
            command = [
                "python", "run_judge.py",
                "--judge_name", "reward_model",
                "--judge_model", checkpoint_path,
                "--pairs", "data/dataset=judgebench,response_model=gpt-4o-2024-05-13.jsonl"
            ]
        elif benchmark == 'reward_bench':
            command = [
                "python", "run_v2.py",
                "--batch_size=1",
                "--model", checkpoint_path,
                "--dataset", "/NAS/zhangsy/datasets/allenai/reward-bench-2"
            ]
        elif benchmark == 'rm_bench':
            command = [
                "python", "run_rm.py",
                "--model", checkpoint_path,
                "--datapath", "data/total_dataset.json",
                "--batch_size", "1",
                "--trust_remote_code",
                "--chat_template", "tulu"
            ]
        
        # è¿è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡º
        result = subprocess.run(
            command,
            cwd=BENCH_DIRS[benchmark],
            env=env,
            capture_output=True,
            text=True
        )
        return result.stdout
    
    except Exception as e:
        print(f"âŒ {benchmark}æ‰§è¡Œå¤±è´¥: {checkpoint_path}\n{str(e)}")
        return None

def parse_judge_output(output):
    """è§£æjudge_benchçš„è¾“å‡º"""
    metrics = {}
    pattern = r'([\w-]+):\s*([\d.]+)%'
    matches = re.findall(pattern, output)
    
    for name, value in matches:
        metrics[name] = float(value)
    
    # ç¡®ä¿å…³é”®æŒ‡æ ‡å­˜åœ¨
    for metric in ['mmlu-pro', 'livebench-reasoning', 'livebench-math', 'livecodebench', 'Overall']:
        metrics.setdefault(metric, None)
    
    # æ·»åŠ æŒ‡æ ‡ç»„
    metrics['Knowledge'] = metrics['mmlu-pro']
    metrics['Math'] = metrics['livebench-math']
    metrics['Coding'] = metrics['livecodebench']
    metrics['Reasoning'] = metrics['livebench-reasoning']
    
    return metrics

def parse_reward_output(output):
    """è§£æreward_benchçš„è¾“å‡º"""
    metrics = {}
    pattern = r'(\w[\w\s]+):\s*[\d\.]+\/[\d.]+\s*$([\d.]+)$'
    matches = re.findall(pattern, output)
    
    for name, value in matches:
        metrics[name.strip()] = float(value)
    
    # æå–æ€»åˆ†æ•°
    ties_match = re.search(r'Ties: Overall score ([\d.]+)', output)
    if ties_match:
        metrics['Ties_Overall'] = float(ties_match.group(1))
    
    # ç¡®ä¿å…³é”®æŒ‡æ ‡å­˜åœ¨
    for metric in ['Factuality', 'Focus', 'Math', 'Precise IF', 'Safety', 'Ties_Overall']:
        metrics.setdefault(metric, None)
    
    return metrics

def parse_rm_output(output):
    """è§£ærm_benchçš„è¾“å‡º"""
    metrics = {}
    pattern = r"'(\w+)': ([\d.]+)"
    matches = re.findall(pattern, output)
    
    for name, value in matches:
        metrics[name] = float(value)
    
    # ç¡®ä¿å…³é”®æŒ‡æ ‡å­˜åœ¨
    for metric in ['chat', 'math', 'code', 'safety', 'hard_acc', 'normal_acc', 'easy_acc', 'total_avg_acc']:
        metrics.setdefault(metric, None)
    
    return metrics

def run_evaluation_for_checkpoint(base_dir, checkpoint_dir, checkpoint_idx, total_checkpoints):
    """è¿è¡Œä¸€ä¸ªcheckpointçš„æ‰€æœ‰è¯„ä¼°ä»»åŠ¡å¹¶ä¿å­˜ç»“æœ"""
    checkpoint_path = os.path.join(base_dir, checkpoint_dir)
    print(f"\nğŸš€ å¤„ç†checkpoint {checkpoint_idx+1}/{total_checkpoints}: {checkpoint_dir}")
    
    # å¹¶è¡Œè¿è¡Œæ‰€æœ‰è¯„ä¼°ä»»åŠ¡
    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(run_benchmark, checkpoint_path, 'judge_bench', 0): 'judge_bench',
            executor.submit(run_benchmark, checkpoint_path, 'reward_bench', 1): 'reward_bench',
            executor.submit(run_benchmark, checkpoint_path, 'rm_bench', 2): 'rm_bench'
        }
        
        all_metrics = {}
        
        for future in futures:
            benchmark = futures[future]
            output = future.result()
            
            if output is None:
                print(f"âš ï¸ {benchmark}æ²¡æœ‰è¾“å‡º: {checkpoint_path}")
                continue
            
            try:
                if benchmark == 'judge_bench':
                    metrics = parse_judge_output(output)
                elif benchmark == 'reward_bench':
                    metrics = parse_reward_output(output)
                elif benchmark == 'rm_bench':
                    metrics = parse_rm_output(output)
                
                all_metrics.update({f"{benchmark}_{k}": v for k, v in metrics.items()})
                print(f"âœ… {benchmark}è§£æå®Œæˆ: {checkpoint_dir}")
                
            except Exception as e:
                print(f"âŒ {benchmark}è§£æå¤±è´¥: {checkpoint_dir}\n{str(e)}")
        
        # ä¿å­˜ç»“æœ
        if all_metrics:
            all_metrics['Checkpoint'] = checkpoint_dir
            
            # ä¿å­˜åˆ°æ±‡æ€»æ–‡ä»¶
            summary_file = os.path.join(base_dir, 'all_benchmarks_summary.csv')
            if os.path.exists(summary_file):
                df = pd.read_csv(summary_file, index_col=0)
                df = pd.concat([df, pd.DataFrame([all_metrics])], ignore_index=True)
            else:
                df = pd.DataFrame([all_metrics])
            
            df.to_csv(summary_file)
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° {summary_file}")
            
            return True
    
    return False

def run_batch_evaluations(base_dir, checkpoint_prefix="checkpoint-"):
    """æ‰¹é‡è¿è¡Œæ‰€æœ‰checkpointçš„è¯„ä¼°"""
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = os.path.join(base_dir, 'benchmark_results')
    os.makedirs(results_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰checkpointç›®å½•
    checkpoint_dirs = sorted(
        d for d in os.listdir(base_dir)
        if d.startswith(checkpoint_prefix) and os.path.isdir(os.path.join(base_dir, d))
    )
    checkpoint_dirs = sorted(checkpoint_dirs, key=lambda s: int(re.search(r'-(\d+)$', s).group(1)))
    
    if not checkpoint_dirs:
        print(f"é”™è¯¯ï¼šåœ¨ {base_dir} ä¸­æœªæ‰¾åˆ°ä»¥'{checkpoint_prefix}'å¼€å¤´çš„checkpointç›®å½•")
        return
    
    total = len(checkpoint_dirs)
    print(f"ğŸ” æ‰¾åˆ° {total} ä¸ªcheckpointï¼Œå¼€å§‹æ‰¹é‡è¯„ä¼°...")
    
    # å¤„ç†æ¯ä¸ªcheckpoint
    for i, checkpoint_dir in enumerate(checkpoint_dirs):
        run_evaluation_for_checkpoint(base_dir, checkpoint_dir, i, total)
    
    print("\nğŸ‰ æ‰€æœ‰checkpointå¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ‰¹é‡è¿è¡Œcheckpointè¯„ä¼°')
    parser.add_argument('--model_dir', type=str, required=True,
                        help='åŒ…å«æ‰€æœ‰checkpointçš„æ¨¡å‹ç›®å½•')
    parser.add_argument('--prefix', type=str, default="checkpoint-",
                        help='checkpointç›®å½•å‰ç¼€')
    args = parser.parse_args()

    if not os.path.isdir(args.model_dir):
        print(f"é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.model_dir}")
        exit(1)

    run_batch_evaluations(
        base_dir=args.model_dir,
        checkpoint_prefix=args.prefix
    )