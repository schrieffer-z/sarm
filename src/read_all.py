import os
import json
import csv
import argparse
import re
from glob import glob

def extract_checkpoint_number(checkpoint_name):
    """ä» checkpoint åç§°ä¸­æå–æ•°å­—éƒ¨åˆ†"""
    match = re.search(r'checkpoint-(\d+)', checkpoint_name)
    if match:
        return int(match.group(1))
    return -1  # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ checkpoint åç§°

def calculate_reward_average(reward_data):
    """è®¡ç®— reward bench çš„å¹³å‡åˆ†ï¼ˆåŸå§‹åˆ†æ•°ï¼‰"""
    scores = []
    for field in ["Factuality", "Focus", "Math", "Precise IF", "Safety", "Ties"]:
        if field in reward_data:
            value = reward_data[field]
            if 0 <= value <= 1:
                scores.append(value * 100)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼
            else:
                scores.append(value)  # å·²ä¸ºç™¾åˆ†æ¯”å½¢å¼
    return sum(scores) / len(scores) if scores else 0

def convert_rm_to_percentage(rm_data):
    """å°† rm bench çš„ç»“æœè½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼"""
    converted = {}
    for key, value in rm_data.items():
        if isinstance(value, (int, float)) and key != "model":
            if 0 <= value <= 1:
                converted[key] = value * 100
            else:
                converted[key] = value
        else:
            converted[key] = value
    return converted

def aggregate_results(base_dir):
    """æ±‡æ€»æ‰€æœ‰ checkpoint çš„ç»“æœå¹¶è®¡ç®—æ•´ä½“å¾—åˆ†"""
    # è·å–æ‰€æœ‰ checkpoint ç›®å½•å¹¶æŒ‰æ•°å­—æ’åº
    checkpoint_dirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and "checkpoint-" in item:
            checkpoint_dirs.append(item_path)
    
    # æŒ‰ checkpoint æ•°å­—æ’åº
    checkpoint_dirs.sort(key=lambda x: extract_checkpoint_number(os.path.basename(x)))
    
    all_results = []
    checkpoint_count = 0
    
    for checkpoint_dir in checkpoint_dirs:
        checkpoint_name = os.path.basename(checkpoint_dir)
        print(f"å¤„ç† checkpoint: {checkpoint_name}")
        
        result = {"checkpoint": checkpoint_name}
        scores = []
        benchmark_data = {}
        
        try:
            # Judge Bench ç»“æœ
            judge_path = os.path.join(checkpoint_dir, "judge_bench.json")
            if os.path.exists(judge_path):
                with open(judge_path) as f:
                    judge_data = json.load(f)
                    result.update({
                        "judge_Coding": judge_data.get("Coding", 0),
                        "judge_Knowledge": judge_data.get("Knowledge", 0),
                        "judge_Math": judge_data.get("Math", 0),
                        "judge_Overall": judge_data.get("Overall", 0),
                        "judge_Reasoning": judge_data.get("Reasoning", 0)
                    })
                    scores.append(judge_data.get("Overall", 0))
                    benchmark_data["judge"] = judge_data
            
            # Reward Bench v2 ç»“æœ
            reward_path = os.path.join(checkpoint_dir, "reward_benchv2.json")
            if os.path.exists(reward_path):
                with open(reward_path) as f:
                    reward_data = json.load(f)
                    reward_avg = calculate_reward_average(reward_data)
                    result["reward_average"] = reward_avg
                    scores.append(reward_avg)
                    
                    # å•ç‹¬å­—æ®µ
                    result.update({
                        "reward_Factuality": reward_data.get("Factuality", 0) * 100,
                        "reward_Focus": reward_data.get("Focus", 0) * 100,
                        "reward_Math": reward_data.get("Math", 0) * 100,
                        "reward_Precise IF": reward_data.get("Precise IF", 0) * 100,
                        "reward_Safety": reward_data.get("Safety", 0) * 100,
                        "reward_Ties": reward_data.get("Ties", 0) * 100
                    })
                    benchmark_data["reward"] = reward_data
            
            # RM Bench ç»“æœ
            rm_path = os.path.join(checkpoint_dir, "rm_bench.json")
            if os.path.exists(rm_path):
                with open(rm_path) as f:
                    rm_data = json.load(f)
                    rm_converted = convert_rm_to_percentage(rm_data)
                    result["rm_total_avg_acc"] = rm_converted.get("total_avg_acc", 0)
                    scores.append(rm_converted.get("total_avg_acc", 0))
                    
                    # å•ç‹¬å­—æ®µ
                    result.update({
                        "rm_chat": rm_converted.get("chat", 0),
                        "rm_code": rm_converted.get("code", 0),
                        "rm_easy_acc": rm_converted.get("easy_acc", 0),
                        "rm_hard_acc": rm_converted.get("hard_acc", 0),
                        "rm_math": rm_converted.get("math", 0),
                        "rm_normal_acc": rm_converted.get("normal_acc", 0),
                        "rm_safety": rm_converted.get("safety", 0)
                    })
                    benchmark_data["rm"] = rm_data
            
            # è®¡ç®—ä¸‰ä¸ª benchmark çš„å¹³å‡åˆ†
            if scores:
                result["benchmark_average"] = sum(scores) / len(scores)
            
            all_results.append(result)
            checkpoint_count += 1
            
        except Exception as e:
            print(f"å¤„ç† {checkpoint_name} æ—¶å‡ºé”™: {str(e)}")
    
    print(f"æˆåŠŸå¤„ç† {checkpoint_count} ä¸ª checkpoint")
    return all_results

def save_results_as_csv(results, base_dir):
    """å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ï¼ˆå®Œå…¨åŒ¹é…å›¾ç‰‡ä¸­çš„è¡¨æ ¼ç»“æ„ï¼‰"""
    if not results:
        return None
    
    # å®šä¹‰å®Œå…¨åŒ¹é…å›¾ç‰‡çš„åˆ—é¡ºåº
    fixed_order = [
        "benchmark_average", 

        # Judge Bench éƒ¨åˆ†
        "judge_Overall",
        "judge_Knowledge",
        "judge_Reasoning",
        "judge_Math",
        "judge_Coding",
        
        # RM Bench éƒ¨åˆ†
        "rm_total_avg_acc",  # Overall
        "rm_chat",            # Chat
        "rm_math",            # Math
        "rm_code",            # Code
        "rm_safety",          # Safety
        "rm_hard_acc", 
        "rm_normal_acc", 
        "rm_easy_acc",
        
        # Reward Bench V2 éƒ¨åˆ†
        "reward_average",     # Overall
        "reward_Factuality",  # Factuality
        "reward_Precise IF",  # Precise IF
        "reward_Math",        # Math
        "reward_Safety",      # Safety
        "reward_Focus",       # Focus
        "reward_Ties",        # Ties
    ]
    
    # åœ¨æœ€å‰é¢æ·»åŠ  checkpoint åˆ—
    final_order = ["checkpoint"] + fixed_order
    
    # åˆ›å»º CSV æ–‡ä»¶
    output_path = os.path.join(base_dir, "benchmark_results.csv")
    with open(output_path, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=final_order)
        writer.writeheader()
        
        # åªå†™å…¥æœ€ç»ˆé¡ºåºä¸­å­˜åœ¨çš„å­—æ®µ
        for result in results:
            filtered_result = {k: v for k, v in result.items() if k in final_order}
            writer.writerow(filtered_result)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    return output_path

def calculate_and_add_overall_average(results, output_path):
    """è®¡ç®—æ‰€æœ‰ checkpoint çš„å¹³å‡å€¼å¹¶æ·»åŠ åˆ° CSV"""
    if not results:
        return {}
    
    # å®šä¹‰å­—æ®µé¡ºåºï¼ˆåŒ¹é… CSV æ–‡ä»¶çš„é¡ºåºï¼‰
    final_order = [
        # Judge Bench
        "judge_Overall", "judge_Knowledge", "judge_Reasoning", "judge_Math", "judge_Coding",
        
        # RM Bench
        "rm_total_avg_acc", "rm_chat", "rm_math", "rm_code", "rm_safety","rm_hard_acc", "rm_normal_acc", "rm_easy_acc"
        
        # Reward Bench V2
        "reward_average", "reward_Factuality", "reward_Precise IF", "reward_Math", "reward_Safety", "reward_Focus", "reward_Ties"
    ]
    
    # è·å–åˆ—åé›†åˆ
    header_fields = ["checkpoint"] + final_order
    
    # è®¡ç®—æ€»ä½“å¹³å‡å€¼
    overall_avg = {}
    for field in final_order:
        if field in results[0]:
            values = [res.get(field, 0) for res in results]
            overall_avg[field] = sum(values) / len(values) if values else 0
    
    # å‡†å¤‡è¦æ·»åŠ çš„è¡Œï¼ˆåªåŒ…å«æœ€ç»ˆé¡ºåºä¸­çš„å­—æ®µï¼‰
    avg_row = {"checkpoint": "Overall Average"}
    for field in final_order:
        avg_row[field] = overall_avg.get(field, "")
    
    # æ·»åŠ ç©ºè¡Œå’Œå¹³å‡å€¼è¡Œåˆ° CSV
    with open(output_path, "a", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header_fields)
        writer.writerow({})  # æ·»åŠ ç©ºè¡Œ
        writer.writerow(avg_row)
    
    print(f"å·²æ·»åŠ æ€»ä½“å¹³å‡å€¼åˆ°: {output_path}")
    return overall_avg


def visualize_comparison(results, overall_avg, base_dir):
    """ç”Ÿæˆå¯è§†åŒ–åˆ†æå›¾è¡¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
    try:
        import matplotlib.pyplot as plt
        import pandas as pd
        import numpy as np
        
        # åˆ›å»º DataFrame
        df = pd.DataFrame(results)
        
        # æå– checkpoint æ•°å­—ç”¨äºæ’åº
        df['checkpoint_num'] = df['checkpoint'].apply(extract_checkpoint_number)
        df = df.sort_values('checkpoint_num')
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = os.path.join(base_dir, "analysis_results")
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. benchmark å¹³å‡åˆ†æ¯”è¾ƒï¼ˆæŒ‰ checkpoint é¡ºåºï¼‰
        plt.figure(figsize=(12, 6))
        if "benchmark_average" in df.columns:
            plt.plot(df["checkpoint_num"], df["benchmark_average"], "o-", markersize=8)
            plt.axhline(y=overall_avg.get("benchmark_average", 0), color="r", linestyle="--", 
                       label=f"Average Value ({overall_avg.get('benchmark_average', 0):.2f}%)")
            
            # æ ‡è®°æœ€é«˜ç‚¹
            max_index = df["benchmark_average"].idxmax()
            max_value = df.loc[max_index, "benchmark_average"]
            plt.annotate(
                f'Max: {max_value:.2f}%', 
                xy=(df.loc[max_index, "checkpoint_num"], max_value),
                xytext=(10, 20), 
                textcoords='offset points',
                arrowprops=dict(arrowstyle="->", color="blue")
            )
            
            plt.title("Model Performance by Training Steps")
            plt.xlabel("Training Steps")
            plt.ylabel("Average Score (%)")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "benchmark_trend_by_checkpoint.png"))
            plt.close()
        
        # 2. å„ä¸ª benchmark è¯¦ç»†åˆ†é¡¹æ¯”è¾ƒ
        plt.figure(figsize=(15, 8))
        if "judge_Overall" in df.columns and "reward_average" in df.columns and "rm_total_avg_acc" in df.columns:
            plt.plot(df["checkpoint_num"], df["judge_Overall"], "o-", markersize=6, label="Judge Bench")
            plt.plot(df["checkpoint_num"], df["reward_average"], "o-", markersize=6, label="Reward Bench")
            plt.plot(df["checkpoint_num"], df["rm_total_avg_acc"], "o-", markersize=6, label="RM Bench")
            
            # æ·»åŠ å¹³å‡çº¿
            plt.axhline(y=overall_avg.get("judge_Overall", 0), color="r", linestyle="--", 
                       alpha=0.7, label=f"Judge Avg ({overall_avg.get('judge_Overall', 0):.2f}%)")
            plt.axhline(y=overall_avg.get("reward_average", 0), color="g", linestyle="--", 
                       alpha=0.7, label=f"Reward Avg ({overall_avg.get('reward_average', 0):.2f}%)")
            plt.axhline(y=overall_avg.get("rm_total_avg_acc", 0), color="b", linestyle="--", 
                       alpha=0.7, label=f"RM Avg ({overall_avg.get('rm_total_avg_acc', 0):.2f}%)")
            
            plt.title("Benchmark Scores by Training Steps")
            plt.xlabel("Training Steps")
            plt.ylabel("Score (%)")
            plt.legend(ncol=2, loc='upper left', bbox_to_anchor=(0, 1))
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "detailed_benchmark_trends.png"))
            plt.close()
        
        # 3. Judge Bench è¯¦ç»†æŒ‡æ ‡åˆ†æ
        plt.figure(figsize=(14, 8))
        judge_cols = [col for col in df.columns if col.startswith("judge_") and col != "judge_Overall"]
        if judge_cols:
            # æå–è¯¦ç»†æŒ‡æ ‡
            judge_metrics = ["Coding", "Knowledge", "Math", "Reasoning"]
            judge_df = df[["checkpoint_num"]].copy()
            for metric in judge_metrics:
                col_name = f"judge_{metric}"
                if col_name in df.columns:
                    judge_df[metric] = df[col_name]
            
            # åˆ›å»ºå›¾è¡¨
            ax = plt.subplot(111)
            for metric in judge_metrics:
                if metric in judge_df.columns:
                    ax.plot(judge_df["checkpoint_num"], judge_df[metric], 
                            marker='o', markersize=5, label=metric)
            
            # æ·»åŠ æ€»ä½“å¹³å‡çº¿
            if "judge_Overall" in df.columns:
                ax.plot(df["checkpoint_num"], df["judge_Overall"], "ko--", 
                       linewidth=2, markersize=8, label="Overall Score")
            
            plt.title("Judge Bench Metrics Analysis")
            plt.xlabel("Training Steps")
            plt.ylabel("Score (%)")
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "judge_bench_metrics_analysis.png"))
            plt.close()
        
        print(f"Generated analysis charts in: {output_dir}")
        
    except ImportError:
        print("Matplotlib library missing, skipping chart generation. Please install with: pip install matplotlib")
    except Exception as e:
        print(f"Error generating charts: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description='æ±‡æ€»æ‰€æœ‰ checkpoint çš„è¯„ä¼°ç»“æœ')
    parser.add_argument('--base_dir', type=str, required=True,
                        help='åŒ…å«æ‰€æœ‰ checkpoint ç›®å½•çš„åŸºç¡€ç›®å½•')
    
    args = parser.parse_args()
    
    if not os.path.isdir(args.base_dir):
        print(f"é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ - {args.base_dir}")
        exit(1)
    
    # æ±‡æ€»ç»“æœ
    results = aggregate_results(args.base_dir)
    
    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœ")
        return
    
    # ä¿å­˜ä¸º CSV
    csv_path = save_results_as_csv(results, args.base_dir)
    
    # è®¡ç®—å¹¶æ·»åŠ æ€»ä½“å¹³å‡å€¼
    overall_avg = calculate_and_add_overall_average(results, csv_path)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    visualize_comparison(results, overall_avg, args.base_dir)
    
    # æ‰“å°æ‘˜è¦
    print("\næ±‡æ€»åˆ†æå®Œæˆ:")
    print(f"- å¤„ç† checkpoint æ•°é‡: {len(results)}")
    if "benchmark_average" in overall_avg:
        print(f"- æ‰€æœ‰ benchmark å¹³å‡åˆ†: {overall_avg['benchmark_average']:.2f}%")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ° benchmark_average å€¼")
    
    # æ‰¾åˆ°æœ€ä½³ checkpoint
    if results and "benchmark_average" in results[0]:
        best_checkpoint = max(results, key=lambda x: x["benchmark_average"])
        print(f"\nğŸ¯ æœ€ä½³è¡¨ç° checkpoint: {best_checkpoint['checkpoint']}")
        print(f"- ç»¼åˆè¯„åˆ†: {best_checkpoint['benchmark_average']:.2f}%")
        if "judge_Overall" in best_checkpoint:
            print(f"- Judge Bench: {best_checkpoint['judge_Overall']:.2f}%")
        if "reward_average" in best_checkpoint:
            print(f"- Reward Bench: {best_checkpoint['reward_average']:.2f}%")
        if "rm_total_avg_acc" in best_checkpoint:
            print(f"- RM Bench: {best_checkpoint['rm_total_avg_acc']:.2f}%")

if __name__ == "__main__":
    main()